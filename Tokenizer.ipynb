{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c9d448-8e1c-4aab-b7a3-3ef7a274f39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26be8d69-b705-4fec-976d-31f9535ead61",
   "metadata": {},
   "source": [
    "## Reading in a short story as text sample into Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe13ed-e41d-4a90-9da1-cc3e7b618843",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ea17f4-3125-4085-af7c-81f77a6592f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of characters : 20479 \n",
      "\n",
      "Sample of first 100 characters :\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no gr\n"
     ]
    }
   ],
   "source": [
    "with open (\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total no. of characters :\", len(raw_text), \"\\n\")\n",
    "print(\"Sample of first 100 characters :\\n\" + str(raw_text[:101]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9e686-cb28-46a5-854d-b01400fd438f",
   "metadata": {},
   "source": [
    "<h2>Testing spliting logic<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "052c18f4-2c6b-48f1-bcf1-28b37f4453ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'do', ' ', 'you', ' ', 'like', ' ', 'tea?', ' ', '<|endoftext|>', ' ', 'In', ' ', 'the', ' ', 'sunlit', ' ', 'terraces', ' ', 'of', ' ', 'the', ' ', 'palace', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d7a1a7c8-ab10-4d36-bd04-768108dbc6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'do', 'you', 'like', 'tea?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8a4260ab-f7de-412f-89ae-320756afad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing white space\n",
      "['Hello', ',', '', ' ', 'World', '.', '', ' ', 'Is', ' ', 'this', ' ', 'a', ' ', 'test', '--', 'text', ' ', '', '?', '', '?', '']\n",
      "After removing white space\n",
      "['Hello', ',', 'World', '.', 'Is', 'this', 'a', 'test', '--', 'text', '?', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, World. Is this a test--text ??\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "print(\"Before removing white space\")\n",
    "print(result)\n",
    "print(\"After removing white space\")\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79f0077d-7908-4fb6-8bca-53ebedae54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--', 'deploring']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cfdd5927-1ce8-4011-ae4f-e0204f9db375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c293fa2-5d37-4f05-9b62-dcb9c6f05378",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bfe4a0dd-ed2a-45b7-b1ac-1d918eacb179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size : 1130\n"
     ]
    }
   ],
   "source": [
    "all_unique_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_unique_words)\n",
    "print(\"vocabulary size :\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e0f2a160-167c-46ba-8747-df86bc69b022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['!', 0],\n",
       " ['\"', 1],\n",
       " [\"'\", 2],\n",
       " ['(', 3],\n",
       " [')', 4],\n",
       " [',', 5],\n",
       " ['--', 6],\n",
       " ['.', 7],\n",
       " [':', 8],\n",
       " [';', 9],\n",
       " ['?', 10],\n",
       " ['A', 11],\n",
       " ['Ah', 12],\n",
       " ['Among', 13],\n",
       " ['And', 14],\n",
       " ['Are', 15],\n",
       " ['Arrt', 16],\n",
       " ['As', 17],\n",
       " ['At', 18],\n",
       " ['Be', 19],\n",
       " ['Begin', 20],\n",
       " ['Burlington', 21],\n",
       " ['But', 22],\n",
       " ['By', 23],\n",
       " ['Carlo', 24],\n",
       " ['Chicago', 25],\n",
       " ['Claude', 26],\n",
       " ['Come', 27],\n",
       " ['Croft', 28],\n",
       " ['Destroyed', 29],\n",
       " ['Devonshire', 30],\n",
       " ['Don', 31],\n",
       " ['Dubarry', 32],\n",
       " ['Emperors', 33],\n",
       " ['Florence', 34],\n",
       " ['For', 35],\n",
       " ['Gallery', 36],\n",
       " ['Gideon', 37],\n",
       " ['Gisburn', 38],\n",
       " ['Gisburns', 39],\n",
       " ['Grafton', 40],\n",
       " ['Greek', 41],\n",
       " ['Grindle', 42],\n",
       " ['Grindles', 43],\n",
       " ['HAD', 44],\n",
       " ['Had', 45],\n",
       " ['Hang', 46],\n",
       " ['Has', 47],\n",
       " ['He', 48],\n",
       " ['Her', 49],\n",
       " ['Hermia', 50],\n",
       " ['His', 51]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {token : integer for integer, token in enumerate(all_unique_words)}\n",
    "vocab_print = [[token, integer] for token, integer in vocab.items()]\n",
    "vocab_print[:52]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c72d3-b172-4a7a-b83a-33e95493b250",
   "metadata": {},
   "source": [
    "<h2>Tokenizer v1 <h2>\n",
    "<h3>Create an class to get a vocabulary, then create a method for encoding which changes text to token id then get a decoder that gets an list of id and returns a string as output <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "43a61678-e1e2-4895-8854-8848e7eccad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {val : key for key, val in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[i] for i in preprocessed]\n",
    "        return ids\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dcb1a382-510a-4f84-b778-db85e82f3d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86f2cd29-f0de-4e06-aac2-a2fed7ac78dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2864ac8-107e-4376-9246-ddde921881e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[47], line 8\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      6\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 8\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[47], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 8\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c4397e-e865-4846-87f8-89de70ce4861",
   "metadata": {},
   "source": [
    "<h3>This error occours because when it encounters an word not in vocubulary it throws an error<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4d478-47dd-4a47-ba09-004a3d5a8e1a",
   "metadata": {},
   "source": [
    "### ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage\n",
    "from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown\n",
    "words.\n",
    "\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the\n",
    "previous section, SimpleTokenizerV2, to support two new tokens, <|unk|>(Unknown word) and\n",
    "<|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a96110be-1825-4892-95b5-b0e2f21b9800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "\n",
    "vocab = {token : integer for integer, token in enumerate(all_tokens)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca2f01-b386-4a6d-a51c-9e8228bd74c2",
   "metadata": {},
   "source": [
    "<h3>Now the new vocabulary size is 1132 including the two new keys<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9014e240-df94-4588-a8cc-d9e5947fab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "younger : 1127\n",
      "your : 1128\n",
      "yourself : 1129\n",
      "<|endoftext|> : 1130\n",
      "<|unk|> : 1131\n"
     ]
    }
   ],
   "source": [
    "for key, val in list(vocab.items())[-5:]:\n",
    "    print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fb0081-5685-4a4b-95d4-36c126afb51d",
   "metadata": {},
   "source": [
    "\n",
    "Step 1: Replace unknown words by <|unk|> tokens\n",
    "    \n",
    "Step 2: Replace spaces before the specified punctuations\n",
    "\n",
    "If the value is not in vocabulary then we add <|unk|> for that token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a659c40a-a680-4182-bf3c-35ccb3ff9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17d1e925-e177-4b1f-9bd2-be042fe19d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ae6943d-f20b-4bcd-b76e-62086cf1ef7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d8f37d7-90ec-4f8f-95b1-e80818b83ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848f592-8382-498d-8f98-7833a8c302b3",
   "metadata": {},
   "source": [
    "Based on comparing the de-tokenized text above with the original input text, we know that\n",
    "the training dataset, Edith Wharton's short story The Verdict, did not contain the words\n",
    "\"Hello\" and \"palace.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b003b-eed7-457a-a795-eaa214d65920",
   "metadata": {},
   "source": [
    "So far, we have discussed tokenization as an essential step in processing text as input to\n",
    "LLMs. Depending on the LLM, some researchers also consider additional special tokens such\n",
    "as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It\n",
    "signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text,\n",
    "and is especially useful when concatenating multiple unrelated texts,\n",
    "similar to <|endoftext|>. For instance, when combining two different\n",
    "Wikipedia articles or books, the [EOS] token indicates where one article\n",
    "ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one,\n",
    "the batch might contain texts of varying lengths. To ensure all texts have\n",
    "the same length, the shorter texts are extended or \"padded\" using the\n",
    "[PAD] token, up to the length of the longest text in the batch.\n",
    "\n",
    "\n",
    "Note that the tokenizer used for GPT models does not need any of these tokens mentioned\n",
    "above but only uses an <|endoftext|> token for simplicity\n",
    "\n",
    "\n",
    "the tokenizer used for GPT models also doesn't use an <|unk|> token for outof-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks\n",
    "down words into subword units\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
